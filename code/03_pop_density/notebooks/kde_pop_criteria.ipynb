{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52bcea0-d1cd-4b37-865d-a13061ac75a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:24:45.463229Z",
     "iopub.status.busy": "2024-09-01T18:24:45.462725Z",
     "iopub.status.idle": "2024-09-01T18:24:45.477323Z",
     "shell.execute_reply": "2024-09-01T18:24:45.476264Z",
     "shell.execute_reply.started": "2024-09-01T18:24:45.463207Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import rasterra as rt\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "import rra_tools\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from affine import Affine\n",
    "from rasterio.fill import fillnodata\n",
    "from scipy.signal import oaconvolve\n",
    "import tqdm\n",
    "import time\n",
    "import os \n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "from rra_tools import parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95f4e6-5d6e-4d09-9541-fffe7f1ace90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T17:59:22.542476Z",
     "iopub.status.busy": "2024-08-19T17:59:22.541825Z",
     "iopub.status.idle": "2024-08-19T17:59:22.547663Z",
     "shell.execute_reply": "2024-08-19T17:59:22.546672Z",
     "shell.execute_reply.started": "2024-08-19T17:59:22.542435Z"
    }
   },
   "source": [
    "# Documentation\n",
    "\n",
    "## Inputs: \n",
    "- Fire perimeters for (1) continental US, (2) Hawaii, (3) Alaska\n",
    "- Global Human Settlement Layer population estimates for 2000, 2005, 2010, 2015, 2020 (100m resolution, Molweide projection). Links in get_data.sh\n",
    "\n",
    "## Analytic overview:\n",
    "\n",
    "This script takes in a dataset of fire perimeters and a gridded population dataset. \n",
    "\n",
    "The parameters are: \n",
    "- Area threshold (area_thresh): this is the threshold by which we determine if a fire is large or small. Fires greater than or equal to 1000 acres are considered large. Based on size, fires get different buffers (one buffer for small fires and one for large). This is parameterized in square-meters.\n",
    "- Large fire buffer (large_fire_buffer) = This is the buffer around the fire perimeter for large (>= 1000 acres) fires, in meters.\n",
    "- Small fire buffer (small_fire_buffer) = This is the buffer around the fire perimeter for small (< 1000 acres) fires, in meters.\n",
    "- Population averaging radius (pop_average_radius): This is the radius for a circle with the area that we are using in our denominator of population density. In other words, if our criteria is per 1 square-kilometer, this is the radius of a circle that has an area of 1 square-kilometer (or 1000 square-meters). This is parameterized in meters.\n",
    "- Population density criteria (pop_density_criteria): This is the number of people per square-meter that are required to make an area a community.\n",
    "\n",
    "The process is: \n",
    "1. It first buffers each fire, with a different buffer size based on whether the fire is big or small. A big fire is defined as a fire that is bigger than 1000 acres (converted to 4046856 meters). Big fires get buffered by 20km and small fires get buffered by 10km. \n",
    "\n",
    "2. Following the buffering of the fire, we create a bounding box around the fire that adds the radius of a `pop_average_radius` circle to each side of the fire. This is to ensure we pull enough of the population raster to do our density calculation correctly.\n",
    "\n",
    "3. Once we have all of this information, we load a subset of our population raster using the bounding box that we created in step 2 to determine the subset loaded.\n",
    "\n",
    "4. Using the population data, we build a kernel for the convolution - this is an average kernel.\n",
    "\n",
    "5. We use our kernel to do a convolution -- for every pixel, we sum all the people within our radius of that pixel (currently 300m). The result of this is a raster of population density averaged to people per square kilometer.\n",
    "\n",
    "6. Now, we can use our buffered fire perimeter to find the maximum value of population density for any individual pixel within the buffered perimeter. If any of them exceed our population density criteria (`pop_density_criteria`), then we determine that the fire overlaps with a community that exceeds our population density threshold and thus meets our population density criteria for the fire overlapping with a community. \n",
    "\n",
    "\n",
    "## Outputs: \n",
    "**Primary output**\n",
    "- CSV with disaster_id and density_criteria_met variable\n",
    "  \n",
    "**Secondary outputs**\n",
    "- Parquet file with metadata (disaster_id, density_criteria_met, state, year, buffer_distance, geometry, crs for that fire's utm)\n",
    "- Diagnostic plots\n",
    "\n",
    "## Next steps:\n",
    "**To discuss with Milo**\n",
    "- File of fires with no acreage (all_disaster_with_ics_poo_no_acreage_cont_us_select_vars) - have point location but not fire size.\n",
    "- There are fires from puerto rico\n",
    "  \n",
    "**Coding next steps**\n",
    "- put into executable python script and make error messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa91c54-4f84-4dfb-9e6b-9d2703866eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:24:49.226831Z",
     "iopub.status.busy": "2024-09-01T18:24:49.225594Z",
     "iopub.status.idle": "2024-09-01T18:24:49.233589Z",
     "shell.execute_reply": "2024-09-01T18:24:49.232888Z",
     "shell.execute_reply.started": "2024-09-01T18:24:49.226794Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"~/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire_disasters_lite/data\").expanduser()\n",
    "plot_dir = Path(\"~/Desktop/Desktop/epidemiology_PhD/02_projects/wildfire-disaster/plots/\").expanduser()\n",
    "# list(data_dir.glob(\"*.parquet\"))\n",
    "# list(data_dir.iterdir())\n",
    "mol_crs = \"ESRI: 54009\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e46ee4-7714-4fb2-a4ed-b415caf33fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:24:50.347651Z",
     "iopub.status.busy": "2024-09-01T18:24:50.343777Z",
     "iopub.status.idle": "2024-09-01T18:24:50.361620Z",
     "shell.execute_reply": "2024-09-01T18:24:50.361204Z",
     "shell.execute_reply.started": "2024-09-01T18:24:50.347575Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def make_convolution_kernel(\n",
    "    pixel_resolution_m: int | float,\n",
    "    radius_m: int | float,\n",
    ") :\n",
    "    radius = int(radius_m // pixel_resolution_m)\n",
    "    y, x = np.ogrid[-radius : radius + 1, -radius : radius + 1]\n",
    "\n",
    "    kernel = (x**2 + y**2 < radius**2).astype(float)\n",
    "    kernel = kernel / kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def make_spatial_average(\n",
    "    tile: rt.RasterArray,\n",
    "    radius: int | float,\n",
    ") :\n",
    "    arr = np.nan_to_num(tile.to_numpy())\n",
    "\n",
    "    kernel = make_convolution_kernel(tile.x_resolution, radius) # tile.x_resolution is pulling the pop raster res, which is 100m in the ghsl data\n",
    "\n",
    "    out_image = oaconvolve(arr, kernel, mode=\"same\")\n",
    "\n",
    "    out_raster = rt.RasterArray(\n",
    "        out_image,\n",
    "        transform=tile.transform,\n",
    "        crs=tile.crs,\n",
    "        no_data_value=tile.no_data_value,\n",
    "    )\n",
    "    return out_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2092a2-d343-41c6-a7a8-d62b48b25e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:24:56.208850Z",
     "iopub.status.busy": "2024-09-01T18:24:56.207285Z",
     "iopub.status.idle": "2024-09-01T18:24:56.355082Z",
     "shell.execute_reply": "2024-09-01T18:24:56.354787Z",
     "shell.execute_reply.started": "2024-09-01T18:24:56.208793Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in fires\n",
    "fires_hi = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_hawaii_select_variables.parquet\")\n",
    "fires_ak = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_alaska_select_variables.parquet\")\n",
    "fires_conus = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_conus_select_variables.parquet\")\n",
    "us_counties = gpd.read_file(\"~/Desktop/Desktop/epidemiology_PhD/01_data/clean/us_cnty_boundaries.geojson\")\n",
    "\n",
    "# read in utm map\n",
    "utm_map = pd.read_csv(data_dir / \"utm_popden.csv\")\n",
    "utm_map['state_list'] = utm_map['states'].apply(lambda s: s.split(','))\n",
    "utm_map = utm_map.explode('state_list').set_index('state_list')['crs'].to_dict()\n",
    "utm_map = {s.strip(): f\"EPSG:{crs}\" for s, crs in utm_map.items()}\n",
    "utm_map[\"PR\"] = \"EPSG:3920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d04b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_conus = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_conus_select_variables.parquet\")\n",
    "fires_conus[fires_conus['disaster_id']== \"2011-06-18_4097_TX_NEWTON_3172\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41152055",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_conus = gpd.read_parquet(data_dir / \"02_processed/fires_conus_pop_density.parquet\")\n",
    "fires_conus['shape'].crs\n",
    "fires_alaska = gpd.read_parquet(data_dir / \"02_processed/fires_alaska_pop_density.parquet\")\n",
    "fires_hi = gpd.read_parquet(data_dir / \"02_processed/fires_hawaii_pop_density.parquet\")\n",
    "fires_conus[fires_conus['disaster_id']== \"2011-06-18_4097_TX_NEWTON_3172\"]\n",
    "\n",
    "# fires_conus.to_file(data_dir / \"02_processed/fires_conus_pop_density.geojson\", driver=\"GeoJSON\")\n",
    "# fires_alaska.to_file(data_dir / \"02_processed/fires_alaska_pop_density.geojson\", driver=\"GeoJSON\")\n",
    "# fires_hi.to_file(data_dir / \"02_processed/fires_hawaii_pop_density.geojson\", driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_alaska\n",
    "value_counts = fires_alaska['density_criteria_met'].value_counts(dropna=False)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38845290",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows = fires_alaska[fires_alaska['density_criteria_met'].isna()]\n",
    "na_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e386ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "large_fire_buffer = 20000 # meters\n",
    "small_fire_buffer = 10000 # meters\n",
    "pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 300 sq m \n",
    "pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "na_rows['shape_area'] = na_rows['shape'].area\n",
    "na_rows['shape_area_km2'] = na_rows['shape_area'] / 1000**2\n",
    "# na_rows[['shape', 'shape_area_km2']]\n",
    "\n",
    "from shapely.validation import make_valid\n",
    "na_rows['shape_fixed'] = na_rows['shape'].apply(make_valid)\n",
    "na_rows['shape_fixed_is_valid'] = na_rows['shape_fixed'].is_valid\n",
    "# print(na_rows[['disaster_id', 'shape_fixed', 'shape_fixed_is_valid']])\n",
    "failed_ids = na_rows['disaster_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fires_alaska\n",
    "fire_dfs = []\n",
    "failed_ids = []\n",
    "area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "large_fire_buffer = 20000 # meters\n",
    "small_fire_buffer = 10000 # meters\n",
    "pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 300 sq m \n",
    "pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "keep_cols = ['disaster_id', 'year', 'states_aggregated_list', 'shape']\n",
    "row_tuples = list(df[keep_cols].itertuples(index=False, name=None))\n",
    "# row_tuples = row_tuples[-89:]\n",
    "for disaster_id, old_year, state_list, fire_poly in tqdm.tqdm(row_tuples):\n",
    "    year = round(old_year / 5)*5\n",
    "    state = state_list[:2]\n",
    "    if fire_poly.is_empty:\n",
    "        failed_ids.append(\n",
    "            (disaster_id, \"empty_geometry\")\n",
    "        )\n",
    "        continue\n",
    "    fire_crs = utm_map[state]\n",
    "    fire_series = gpd.GeoSeries([fire_poly], crs=df.crs).to_crs(fire_crs)\n",
    "    if not fire_series.is_valid.iloc[0]:\n",
    "        print(disaster_id, 'is invalid')\n",
    "        failed_ids.append(\n",
    "            (disaster_id, \"invalid_geometry\")\n",
    "        )\n",
    "        continue\n",
    "    buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "    buffered_fire_series = fire_series.buffer(buffer_dist) # buffer dist in meters\n",
    "    \n",
    "    bounding_box = buffered_fire_series.envelope.buffer(pop_average_radius*1.1).to_crs(mol_crs).iloc[0]\n",
    "\n",
    "    if bounding_box.area/1000**2 > 1_000_000: # 25k sq kilometers (2x the biggest fire in the us)\n",
    "        failed_ids.append(\n",
    "            (disaster_id, \"bounding_box_too_large\")\n",
    "        )\n",
    "        print(\n",
    "            f\"Disaster ID {disaster_id}: Bounding box too large. \"\n",
    "            f\"Bounding box area = {bounding_box.area / 1000**2:.2f} sq km, \"\n",
    "            f\"Buffered fire area = {buffered_fire_series.area.iloc[0] / 1000**2:.2f} sq km, \"\n",
    "            f\"Original fire area = {fire_series.area.iloc[0] / 1000**2:.2f} sq km\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # II. load pop data \n",
    "    try:\n",
    "        pop = rt.load_raster(data_dir / f\"01_raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(fire_crs)\n",
    "    except:\n",
    "        import pdb; pdb.set_trace()\n",
    "    pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "\n",
    "    # III. build kernel and do convolution with kernel - for every pixel, sum all the people within a kilometer of that pixel,\n",
    "    # so kernel should be all 1's and should be 10 pixels wide and 10 pixels tall\n",
    "    # that convolution gives back a raster of pop density averaged to people per sq km\n",
    "    mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "\n",
    "    # IV. determine if this fire meets density criteria by: \n",
    "    # i. take buffered fire poly and mask everything outside of that (set everything outside buffered poly to 0 which you can do w/ raster.mask)\n",
    "    # ii. find max pixel val and if it exceeds your threshold then it overlaps with a communtiy that exceeds the threshold and is marked as TRUE in final csv. \n",
    "    max_pop_density = np.max(mean_pop_density.mask(buffered_fire_series).to_numpy())\n",
    "    density_criteria_met = max_pop_density > pop_density_criteria\n",
    "\n",
    "    # V. add to results df \n",
    "    df_fire = pd.DataFrame({\n",
    "        'disaster_id': [str(disaster_id)],\n",
    "        'density_criteria_met': [density_criteria_met],\n",
    "        'max_pop_density': [max_pop_density],\n",
    "        'state': [state],\n",
    "        'year': [year],\n",
    "        'buffer_distance': [buffer_dist],\n",
    "        'geometry': [fire_series.iloc[0]],\n",
    "        'crs': [fire_crs],\n",
    "    })\n",
    "    fire_dfs.append(df_fire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a347aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for disaster_id, reason in failed_ids:\n",
    "    if reason == \"bounding_box_too_large\":\n",
    "        print(f\"Disaster ID {disaster_id}: Bounding box exceeds allowable area.\")\n",
    "    elif reason == \"invalid_geometry\":\n",
    "        print(f\"Disaster ID {disaster_id}: Invalid geometry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera.typing import Series\n",
    "from pandera.typing.geopandas import GeoSeries\n",
    "import json\n",
    "from shapely import Polygon, MultiPolygon\n",
    "from pprint import pprint\n",
    "\n",
    "class InputSchema(pa.DataFrameModel):\n",
    "    disaster_id: Series[str]\n",
    "    year: Series[int] = pa.Field(ge=2000)\n",
    "    states_aggregated_list: Series[str]\n",
    "    shape: GeoSeries\n",
    "\n",
    "    @pa.check(\"shape\")\n",
    "    def shape_type_check(cls, shape: GeoSeries) -> Series[bool]:\n",
    "        return shape.apply(lambda x: isinstance(x, (Polygon, MultiPolygon)))\n",
    "\n",
    "    @pa.check(\"shape\")\n",
    "    def shape_area_check(cls, shape: GeoSeries) -> Series[bool]:\n",
    "        return shape.area.gt(0) & shape.area.lt(1e12) # figure out area threshold number\n",
    "\n",
    "\n",
    "    \n",
    "fires_conus = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_conus_select_variables.parquet\")\n",
    "try:\n",
    "    InputSchema.validate(fires_conus, lazy=True)\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    errors_conus = e\n",
    "\n",
    "fires_ak = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_alaska_select_variables.parquet\")\n",
    "try:\n",
    "    InputSchema.validate(fires_ak, lazy=True)\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    errors_ak = e\n",
    "\n",
    "\n",
    "fires_hi = gpd.read_parquet(data_dir / \"01_raw/all_disaster_perimeters_ics_and_news_buffers_hawaii_select_variables.parquet\")\n",
    "try:\n",
    "    InputSchema.validate(fires_hi, lazy=True)\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    errors_hi = e\n",
    "\n",
    "\n",
    "# TODO: make a function that reads a file and runs the validater "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcf626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_conus.failure_cases.groupby([\"column\", \"check\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ak.failure_cases.groupby([\"column\", \"check\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_hi.failure_cases.groupby([\"column\", \"check\"]).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04441683-5f04-4068-9322-73b4a2054483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:24:57.102441Z",
     "iopub.status.busy": "2024-09-01T18:24:57.101634Z",
     "iopub.status.idle": "2024-09-01T18:32:38.724382Z",
     "shell.execute_reply": "2024-09-01T18:32:38.724075Z",
     "shell.execute_reply.started": "2024-09-01T18:24:57.102401Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "large_fire_buffer = 20000 # meters\n",
    "small_fire_buffer = 10000 # meters\n",
    "pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 300 sq m \n",
    "pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "\n",
    "fire_dfs = []\n",
    "for df in [fires_conus, fires_ak, fires_hi]: \n",
    "    # df['disaster_id'] = df['disaster_id']\n",
    "    df['state'] = df['states_aggregated_list'].apply(lambda s: s[:2])\n",
    "    df['year'] = df['year'].apply(lambda y: round(y / 5)*5)\n",
    "    keep_cols = ['disaster_id', 'year', 'state', 'shape']\n",
    "    row_tuples = list(df[keep_cols].itertuples(index=False, name=None))\n",
    "    row_tuples = row_tuples[-89:]\n",
    "    import pdb; pdb.set_trace()\n",
    "    for disaster_id, year, state, fire_poly in tqdm.tqdm(row_tuples):\n",
    "        fire_crs = utm_map[state]\n",
    "        fire_series = gpd.GeoSeries([fire_poly], crs=df.crs).to_crs(fire_crs)\n",
    "        \n",
    "        # I. buffer fire\n",
    "         # i. convert fire to appropriate UTM\n",
    "         # ii. calc area of fire poly\n",
    "         # iii. create a perimeter around the fire \n",
    "           # large fire is if area > 20000\n",
    "           # buffer_dist = ifelse(size == \"large_fire\", 20000, 10000))\n",
    "         # iv. create a bounding box around that perimeter\n",
    "           # buffer bounding box by half the radius of the kernel over which we are estimating density (so plus 1000/sqrt(pi))\n",
    "         # v. convert the buffered bounding box to molweide projection\n",
    "        buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "        buffered_fire_series = fire_series.buffer(buffer_dist) # buffer dist in meters\n",
    "        \n",
    "        bounding_box = buffered_fire_series.envelope.buffer(pop_average_radius*1.1).to_crs(mol_crs).iloc[0]\n",
    "\n",
    "        # II. load pop data \n",
    "        pop = rt.load_raster(data_dir / f\"01_raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(fire_crs)\n",
    "        pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "\n",
    "        # III. build kernel and do convolution with kernel - for every pixel, sum all the people within a kilometer of that pixel,\n",
    "          # so kernel should be all 1's and should be 10 pixels wide and 10 pixels tall\n",
    "          # that convolution gives back a raster of pop density averaged to people per sq km\n",
    "        mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "\n",
    "        # IV. determine if this fire meets density criteria by: \n",
    "          # i. take buffered fire poly and mask everything outside of that (set everything outside buffered poly to 0 which you can do w/ raster.mask)\n",
    "          # ii. find max pixel val and if it exceeds your threshold then it overlaps with a communtiy that exceeds the threshold and is marked as TRUE in final csv. \n",
    "        max_pop_density = np.max(mean_pop_density.mask(buffered_fire_series).to_numpy())\n",
    "        density_criteria_met = max_pop_density > pop_density_criteria\n",
    "\n",
    "        # V. add to results df \n",
    "        df_fire = pd.DataFrame({\n",
    "            'disaster_id': [disaster_id],\n",
    "            'density_criteria_met': [density_criteria_met],\n",
    "            'max_pop_density': [max_pop_density],\n",
    "            'state': [state],\n",
    "            'year': [year],\n",
    "            'buffer_distance': [buffer_dist],\n",
    "            'geometry': [fire_series.iloc[0]],\n",
    "            'crs': [fire_crs],\n",
    "            \n",
    "        })\n",
    "        fire_dfs.append(df_fire)\n",
    "\n",
    "df = pd.concat(fire_dfs, ignore_index = True)\n",
    "fires_included_prop = round(len(df[df[\"density_criteria_met\"] == True])/len(df)*100, 2)\n",
    "df['geometry'] = df['geometry'].apply(lambda geom: geom.wkt)\n",
    "df.to_parquet(data_dir / \"02_processed/fire_pop_density_criteria.parquet\")\n",
    "df[[\"disaster_id\", \"density_criteria_met\"]].to_csv(data_dir / \"02_processed/fire_pop_density_criteria.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca8eac-6edb-461c-b5d9-8b42c364d8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:32:51.299201Z",
     "iopub.status.busy": "2024-09-01T18:32:51.298816Z",
     "iopub.status.idle": "2024-09-01T18:32:51.519476Z",
     "shell.execute_reply": "2024-09-01T18:32:51.519027Z",
     "shell.execute_reply.started": "2024-09-01T18:32:51.299177Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(data_dir / \"02_processed/fires_conus_pop_density.parquet\")\n",
    "df.columns\n",
    "['wildfire_id' = 'disaster_id', \n",
    " 'wildfire_year' = 'year',\n",
    " 'wildfire_states' = 'states_aggregated_list',\n",
    " 'wildfire_counties' = 'counties_aggregated_list',\n",
    " 'wildfire_area' = 'burn_zone_area_sq_m', # change to km and divide by 1000 twice :) \n",
    " 'wildfire_fatalities'  = 'fatalities_civ_flex_max', # after 2014, this should be populated, before 2014 it will be NA \n",
    " 'wildfire_civil_fatalities' = 'fatalities_civilian_max', # before 2014 this should be populated, after 2014 it will be NA\n",
    " 'wildfire_struct_destroyed' = '', # SEE NOTES, either want row 70 or 71.\n",
    " 'wildfire_community_intersect' = 'density_criteria_met',\n",
    " 'wildfire_fema_dec' = 'fema_declaration_string_aggregated_list',\n",
    " 'wildfire_ignition_date' = '', # use row 25 if it is there, if missing then use row 28\n",
    " 'wildfire_fema_dec_date_start' = '', # EDITED VAR NAME! fema_declaration_date_aggregated_min\n",
    " 'wildfire_fema_dec_date_end' = '', # NEW VAR! fema_disaster_closeout_date_aggregated_min (but confirm that row 41 is generally later than 43, if not go back to drawing board)\n",
    " 'wildfire_end_date' = '', # use ics_209_fod_contain_doy_date_min_aggregated_min for everything except CA (but check for missingness) and for CA use redbook. use the later date and see if that is the max or the min. \n",
    " 'wildfire_complex' = '', # if complex_names_list_aggregated_list is not NA, then this is TRUE \n",
    " 'wildfire_complex_names' = '', # NEW VAR!  row 46 (complex_names_list_aggregated_list)\n",
    " 'wildfire_poo_lat' = '', # use row 19, if missing then use 21/22\n",
    " 'wildfire_poo_lon' = '', # use row 19, if missing then use 21/22\n",
    " 'geometry_src' = '', # use row 9 and if there are multiple datasets listed, first use mtbs, then fired, then usgs, then geomac, then nifc, then ics vs newspaper if there was only poo not geometry\n",
    " 'geometry' = 'shape',\n",
    " 'redbook_id' = '', # row 6\n",
    " 'ics_id' = '', # row 4\n",
    " 'fired_id' = '', # check manuscript to see if it has an id\n",
    " 'mtbs_id' = '', # use row 10 but confirm with manuscript to make sure milo didnt just make it \n",
    " 'nifc_id' = '', # confirm if this has an id in the manuscript \n",
    " 'fema_id' = '', # NEW VAR! row 8 \n",
    " 'geomac_id' = '', # NEW VAR! row 12 (wont exist in future)\n",
    " 'usgs_id' = '', # NEW VAR! row 11 (wont exist in future)\n",
    "]\n",
    "# WRITE UP DECISIONS ON VARS FOR MANUSCRIPT AND ADD TO VERSION JOAN SENT \n",
    "# MOVE COMPLEX FROM DATE SECTION OF DATA DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(data_dir / \"02_processed/fires_conus_pop_density.parquet\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93181529-c947-488a-bf48-7630dc76c89a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:33:56.228714Z",
     "iopub.status.busy": "2024-09-01T18:33:56.228089Z",
     "iopub.status.idle": "2024-09-01T18:33:56.243996Z",
     "shell.execute_reply": "2024-09-01T18:33:56.243285Z",
     "shell.execute_reply.started": "2024-09-01T18:33:56.228685Z"
    }
   },
   "outputs": [],
   "source": [
    "# plots \n",
    "def plot_fires(disaster_id): \n",
    "    area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "    large_fire_buffer = 20000 # meters\n",
    "    small_fire_buffer = 10000 # meters\n",
    "    pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 1 sq km \n",
    "    pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "    \n",
    "    cmap = \"seismic\"\n",
    "    df_plot = pd.read_parquet(data_dir / \"02_processed/fire_pop_density_criteria.parquet\").iloc[0]\n",
    "    df_plot = df[df[\"disaster_id\"] == disaster_id].iloc[0]\n",
    "    fire_poly = wkt.loads(df_plot[\"geometry\"])\n",
    "    crs = df_plot['crs']\n",
    "    year = df_plot['year']\n",
    "    \n",
    "    fire_series = gpd.GeoSeries([fire_poly], crs=crs)\n",
    "    buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "    buffered_fire_series = fire_series.buffer(buffer_dist)\n",
    "    fire_poly_plus_buffer = gpd.GeoSeries([fire_poly, buffered_fire_series.iloc[0]], crs=crs)\n",
    "    bounding_box = buffered_fire_series.envelope.buffer(pop_average_radius * 1.1).to_crs(mol_crs).iloc[0]\n",
    "    pop = rt.load_raster(data_dir / f\"01_raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(crs)\n",
    "    pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "    mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "    \n",
    "    # gridspec layout \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "    \n",
    "    # fig 1: fire poly, buffered fire poly, pop\n",
    "    pop_transformed = pop*100/pop_density_criteria\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    pop_transformed.plot(ax=ax1, cmap = cmap, vmin = 1e-4, vmax = 2, under_color = \"lightgrey\")\n",
    "    fire_poly_plus_buffer.boundary.plot(ax=ax1, linewidth=1.5, edgecolor = 'limegreen')\n",
    "    ax1.set_title(f\"Population counts\", fontsize=16)\n",
    "    ax1.set_axis_off()\n",
    "    \n",
    "    # fig 2: fire poly, buffered fire poly, pop density\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    pop_density_transformed = pop_density/pop_density_criteria # BUG!\n",
    "    pop_density_transformed.plot(ax=ax2, cmap = cmap, vmin = 1e-4, vmax = 2, under_color = \"lightgrey\")\n",
    "    fire_poly_plus_buffer.boundary.plot(ax=ax2, linewidth=1.5, edgecolor = 'limegreen')\n",
    "    ax2.set_title(f\"Population density\", fontsize=16)\n",
    "    ax2.set_axis_off()\n",
    "    \n",
    "    fig.suptitle(f\"Fire polygon with buffer (buffer distance: {buffer_dist} meters) \\nDisaster ID: {df_plot['disaster_id']}, State: {df_plot['state']}, Year: {df_plot['year']} \\nCriteria met: {df_plot[\"density_criteria_met\"]}\", fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    pdf_filename = plot_dir / f\"fire_pop_dens_{df_plot['disaster_id']}.pdf\"\n",
    "    plt.savefig(pdf_filename, format='pdf')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026f445-6102-4a7e-8be2-65ab0b399fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:33:57.042831Z",
     "iopub.status.busy": "2024-09-01T18:33:57.041879Z",
     "iopub.status.idle": "2024-09-01T18:33:57.471085Z",
     "shell.execute_reply": "2024-09-01T18:33:57.470828Z",
     "shell.execute_reply.started": "2024-09-01T18:33:57.042782Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[2, 1])\n",
    "\n",
    "df_fig = df[df[\"max_pop_density\"]<100000]\n",
    "# fig 1: histogram of max_pop_density for all fires\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "ax1.hist(df_fig['max_pop_density'], bins=150, color='firebrick', edgecolor='black')\n",
    "ax1.set_title('Histogram of maximum population density for all fires, subset to density < 100000')\n",
    "ax1.set_xlabel('Max population density')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# fig 2: histogram of max_pop_density where density_criteria_met = True\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(df_fig[df_fig['density_criteria_met'] == True]['max_pop_density'], bins=150, color='salmon', edgecolor='black')\n",
    "ax2.set_title('Max population density, subset to density < 100000 (criteria met)')\n",
    "ax2.set_xlabel('Max population density')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "# fig 3: histogram of max_pop_density where density_criteria_met = False\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.hist(df_fig[df_fig['density_criteria_met'] == False]['max_pop_density'], bins=150, color='peachpuff', edgecolor='black')\n",
    "ax3.set_title('Max population density (criteria not met)')\n",
    "ax3.set_xlabel('Max population density')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"Population density histograms (percent included = {fires_included_prop}%)\", fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "pdf_filename = plot_dir / f\"histograms.pdf\"\n",
    "plt.savefig(pdf_filename, format='pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ad044-f2fd-4212-ac3d-a26e1f4a65fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T18:33:58.322224Z",
     "iopub.status.busy": "2024-09-01T18:33:58.321643Z",
     "iopub.status.idle": "2024-09-01T18:33:58.738167Z",
     "shell.execute_reply": "2024-09-01T18:33:58.737752Z",
     "shell.execute_reply.started": "2024-09-01T18:33:58.322193Z"
    }
   },
   "outputs": [],
   "source": [
    "for disaster in tqdm.tqdm(disaster_ids): \n",
    "    plot_fires(disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b5253-394c-49f3-875c-3b6138c731ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T04:39:24.760519Z",
     "iopub.status.busy": "2024-08-20T04:39:24.759747Z",
     "iopub.status.idle": "2024-08-20T04:40:34.319551Z",
     "shell.execute_reply": "2024-08-20T04:40:34.319247Z",
     "shell.execute_reply.started": "2024-08-20T04:39:24.760468Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_dir = plot_dir\n",
    "pdf_files = sorted(pdf_dir.glob(\"fire_pop_dens_*.pdf\"))\n",
    "histogram_pdf = pdf_dir / \"histograms.pdf\"\n",
    "output_pdf = pdf_dir / \"compiled/combined_fire_plots.pdf\"\n",
    "pdf_merger = PyPDF2.PdfMerger()\n",
    "\n",
    "# histogram first\n",
    "with open(histogram_pdf, 'rb') as f:\n",
    "    pdf_merger.append(f)\n",
    "    \n",
    "# then loop through each fire's pdf\n",
    "for pdf_file in pdf_files:\n",
    "    with open(pdf_file, 'rb') as f:\n",
    "        pdf_merger.append(f)\n",
    "with open(output_pdf, 'wb') as output_file:\n",
    "    pdf_merger.write(output_file)\n",
    "pdf_merger.close()\n",
    "print(f\"Combined PDF saved as {output_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0ec6e-0f9b-4347-9507-61f27a5b860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plots \n",
    "\n",
    "# def plot_fires(disaster_id): \n",
    "#     cmap = \"RdGy\"\n",
    "#     df_plot = pd.read_parquet(data_dir / \"processed/fire_pop_density_criteria.parquet\").iloc[0]\n",
    "#     df_plot = df[df[\"disaster_id\"] == disaster_id].iloc[0]\n",
    "#     fire_poly = wkt.loads(df_plot[\"geometry\"])\n",
    "#     crs = df_plot['crs']\n",
    "#     fire_series = gpd.GeoSeries([fire_poly], crs=crs)\n",
    "#     buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "#     buffered_fire_series = fire_series.buffer(buffer_dist)\n",
    "#     fire_poly_plus_buffer = gpd.GeoSeries([fire_poly, buffered_fire_series.iloc[0]], crs=crs)\n",
    "    \n",
    "#     # gridspec layout \n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "#     gs = gridspec.GridSpec(2, 3, figure=fig, width_ratios=[1, 1, 2], height_ratios=[1, 1])\n",
    "    \n",
    "#     # fig 1: fire_poly\n",
    "#     ax1 = fig.add_subplot(gs[0, 0])\n",
    "#     fire_series.plot(ax=ax1, edgecolor='firebrick', facecolor='none')\n",
    "#     ax1.set_title(f\"Fire Polygon\", fontsize=16)\n",
    "#     ax1.title.set_position([0.5, 1.05])\n",
    "#     ax1.set_axis_off()\n",
    "    \n",
    "#     # fig 2: fire poly with buffer\n",
    "#     ax2 = fig.add_subplot(gs[0, 1])\n",
    "#     fire_poly_plus_buffer.boundary.plot(ax=ax2, linewidth=1.5, edgecolor = 'firebrick')\n",
    "#     ax2.set_title(f\"Fire Polygon with Buffer (Buffer Distance: {buffer_dist} meters)\", fontsize=16)\n",
    "#     ax2.title.set_position([0.5, 1.05])\n",
    "#     ax2.set_axis_off()\n",
    "    \n",
    "#     # fig 3: population with fire poly\n",
    "#     bounding_box = buffered_fire_series.envelope.buffer(pop_density_radius * 1.1).to_crs(mol_crs).iloc[0]\n",
    "#     pop = rt.load_raster(data_dir / f\"raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box)\n",
    "#     ax3 = fig.add_subplot(gs[1, 0])\n",
    "#     pop.plot(ax=ax3, cmap='viridis')\n",
    "#     fire_series.to_crs(mol_crs).boundary.plot(ax=ax3, color='firebrick')\n",
    "#     ax3.set_title(\"Population density and fire polygon\", fontsize=16)\n",
    "#     ax3.set_axis_off()\n",
    "    \n",
    "#     # fig 4: pop density in box\n",
    "#     pop_density = make_spatial_sum(pop, pop_density_radius)\n",
    "#     ax4 = fig.add_subplot(gs[1, 1])\n",
    "#     pop_density.plot(ax=ax4, cmap='plasma')\n",
    "#     ax4.set_title(\"Population density in bounding box\", fontsize=16)\n",
    "#     ax4.set_axis_off()\n",
    "    \n",
    "#     # fig 5: pop density in fire poly \n",
    "#     ax5 = fig.add_subplot(gs[:, 2])\n",
    "#     pop_density.mask(fire_series.to_crs(mol_crs)).plot(ax=ax5, cmap='plasma')\n",
    "#     ax5.set_title(\"Population density within fire polygon\", fontsize=16)\n",
    "#     ax5.set_axis_off()\n",
    "#     ax5.set_xlim(fire_series.to_crs(mol_crs).total_bounds[[0, 2]])\n",
    "#     ax5.set_ylim(fire_series.to_crs(mol_crs).total_bounds[[1, 3]])\n",
    "    \n",
    "#     fig.suptitle(f\"Population density criteria for wildfires \\nDisaster ID: {df_plot['disaster_id']}, State: {df_plot['state']}, Year: {df_plot['year']} \\nCriteria met: {df_plot[\"density_criteria_met\"]}\", fontsize=20)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     pdf_filename = plot_dir / f\"fire_pop_dens_{df_plot['disaster_id']}.pdf\"\n",
    "#     plt.savefig(pdf_filename, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79f22e-64c9-4aed-bb78-9d996fe9e06d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:12:29.802578Z",
     "iopub.status.busy": "2024-08-20T03:12:29.802126Z",
     "iopub.status.idle": "2024-08-20T03:12:29.806318Z",
     "shell.execute_reply": "2024-08-20T03:12:29.805870Z",
     "shell.execute_reply.started": "2024-08-20T03:12:29.802548Z"
    }
   },
   "outputs": [],
   "source": [
    "30/300**2 * 1000**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47627e5-fc06-4a3b-a9f3-6dc178446fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea8589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33705426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b28dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from scipy.signal import oaconvolve\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterra as rt\n",
    "import argparse\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "\n",
    "import warnings\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "#-----------------\n",
    "# helper functions\n",
    "def make_convolution_kernel(\n",
    "    pixel_resolution_m: int | float,\n",
    "    radius_m: int | float,\n",
    ") :\n",
    "    radius = int(radius_m // pixel_resolution_m)\n",
    "    y, x = np.ogrid[-radius : radius + 1, -radius : radius + 1]\n",
    "\n",
    "    kernel = (x**2 + y**2 < radius**2).astype(float)\n",
    "    kernel = kernel / kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def make_spatial_average(\n",
    "    tile: rt.RasterArray,\n",
    "    radius: int | float,\n",
    ") :\n",
    "    arr = np.nan_to_num(tile.to_numpy())\n",
    "\n",
    "    kernel = make_convolution_kernel(tile.x_resolution, radius) # tile.x_resolution is pulling the pop raster res, which is 100m in the ghsl data\n",
    "\n",
    "    out_image = oaconvolve(arr, kernel, mode=\"same\")\n",
    "\n",
    "    out_raster = rt.RasterArray(\n",
    "        out_image,\n",
    "        transform=tile.transform,\n",
    "        crs=tile.crs,\n",
    "        no_data_value=tile.no_data_value,\n",
    "    )\n",
    "    return out_raster\n",
    "\n",
    "\n",
    "mol_crs = \"ESRI: 54009\"\n",
    "data_dir = Path(\"/Users/laurenwilner/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire_disasters_lite/data\")\n",
    "\n",
    "#-----------------\n",
    "# read in data\n",
    "# fires\n",
    "df = gpd.read_file(\"~/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire_disasters_lite/data/02_processed/wflite.geojson\")\n",
    "df['wildfire_id'] = range(1, len(df) + 1)\n",
    "df['wildfire_states'] = \"CA\"\n",
    "df['wildfire_year'] = df['wildfire_year'].astype(int)\n",
    "df = df[df['wildfire_year']<2020]\n",
    "\n",
    "# utm map\n",
    "utm_map = pd.read_csv(\"~/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire_disasters_lite/data/utm_popden.csv\")\n",
    "utm_map['state_list'] = utm_map['states'].apply(lambda s: s.split(','))\n",
    "utm_map = utm_map.explode('state_list').set_index('state_list')['crs'].to_dict()\n",
    "utm_map = {s.strip(): f\"EPSG:{crs}\" for s, crs in utm_map.items()}\n",
    "utm_map[\"PR\"] = \"EPSG:3920\"\n",
    "\n",
    "# warnings.simplefilter(\"error\", category=RuntimeWarning)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_ids = []\n",
    "keep_cols = ['wildfire_id', 'wildfire_year', 'wildfire_states', 'geometry']\n",
    "row_tuples = list(df[keep_cols].itertuples(index=False, name=None))\n",
    "area_thresh=1000\n",
    "large_fire_buffer = 20000\n",
    "small_fire_buffer = 10000\n",
    "pop_average_area = 300\n",
    "pop_density_criteria = 96\n",
    "pop_average_radius =pop_average_area/np.sqrt(np.pi)\n",
    "area_thresh = area_thresh*4046.856\n",
    "\n",
    "row_tuples = row_tuples[-5:]\n",
    "fire_dfs = []\n",
    "for wildfire_id, wildfire_year, wildfire_state, fire_poly in tqdm.tqdm(row_tuples):\n",
    "    year = round(wildfire_year / 5)*5\n",
    "    state = wildfire_state[:2]\n",
    "    if fire_poly.is_empty:\n",
    "        failed_ids.append(\n",
    "            (wildfire_id, \"empty_geometry\")\n",
    "        )\n",
    "        continue\n",
    "    fire_crs = utm_map[state]\n",
    "    fire_series = gpd.GeoSeries([fire_poly], crs=df.crs).to_crs(fire_crs)\n",
    "    if not fire_series.is_valid.iloc[0]:\n",
    "        print(wildfire_id, 'is invalid')\n",
    "        failed_ids.append(\n",
    "            (wildfire_id, \"invalid_geometry\")\n",
    "        )\n",
    "        fire_series.iloc[0] = make_valid(fire_series.iloc[0])\n",
    "\n",
    "    buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "    buffered_fire_series = fire_series.buffer(buffer_dist) # buffer dist in meters\n",
    "    \n",
    "    bounding_box = buffered_fire_series.envelope.buffer(pop_average_radius*1.1).to_crs(mol_crs).iloc[0]\n",
    "\n",
    "    if bounding_box.area/1000**2 > 100_000: # 100k sq kilometers\n",
    "        failed_ids.append(\n",
    "            (wildfire_id, \"bounding_box_too_large\")\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # II. load pop data \n",
    "    # try:\n",
    "    pop = rt.load_raster(data_dir / f\"01_raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(fire_crs)\n",
    "    pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "\n",
    "    # III. build kernel and do convolution with kernel - for every pixel, sum all the people within a kilometer of that pixel,\n",
    "    # so kernel should be all 1's and should be 10 pixels wide and 10 pixels tall\n",
    "    # that convolution gives back a raster of pop density averaged to people per sq km\n",
    "    mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "\n",
    "    # IV. determine if this fire meets density criteria by: \n",
    "    # i. take buffered fire poly and mask everything outside of that (set everything outside buffered poly to 0 which you can do w/ raster.mask)\n",
    "    # ii. find max pixel val and if it exceeds your threshold then it overlaps with a communtiy that exceeds the threshold and is marked as TRUE in final csv. \n",
    "    max_pop_density = np.max(mean_pop_density.mask(buffered_fire_series).to_numpy())\n",
    "    wildfire_community_intersect = max_pop_density > pop_density_criteria\n",
    "\n",
    "    # V. add to results df \n",
    "    df_fire = pd.DataFrame({\n",
    "        'wildfire_id': [str(wildfire_id)],\n",
    "        'wildfire_community_intersect': [wildfire_community_intersect]\n",
    "    })\n",
    "    fire_dfs.append(df_fire)\n",
    "    df_out = pd.concat(fire_dfs, ignore_index = True)\n",
    "\n",
    "    df_out.to_csv(data_dir / \"02_processed/fire_pop_density_criteria.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b339e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

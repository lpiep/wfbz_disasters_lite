{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52bcea0-d1cd-4b37-865d-a13061ac75a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T20:59:55.901934Z",
     "iopub.status.busy": "2024-08-22T20:59:55.901649Z",
     "iopub.status.idle": "2024-08-22T20:59:57.169253Z",
     "shell.execute_reply": "2024-08-22T20:59:57.168764Z",
     "shell.execute_reply.started": "2024-08-22T20:59:55.901909Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import rasterra as rt\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "import rra_tools\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from affine import Affine\n",
    "from rasterio.fill import fillnodata\n",
    "from scipy.signal import oaconvolve\n",
    "import tqdm\n",
    "import time\n",
    "import os \n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "from rra_tools import parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95f4e6-5d6e-4d09-9541-fffe7f1ace90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T17:59:22.542476Z",
     "iopub.status.busy": "2024-08-19T17:59:22.541825Z",
     "iopub.status.idle": "2024-08-19T17:59:22.547663Z",
     "shell.execute_reply": "2024-08-19T17:59:22.546672Z",
     "shell.execute_reply.started": "2024-08-19T17:59:22.542435Z"
    }
   },
   "source": [
    "# Documentation\n",
    "# testing change\n",
    "\n",
    "## Inputs: \n",
    "- Fire perimeters for (1) continental US, (2) Hawaii, (3) Alaska\n",
    "- Global Human Settlement Layer population estimates for 2000, 2005, 2010, 2015, 2020 (100m resolution, Molweide projection). Links in get_data.sh\n",
    "\n",
    "## Analytic overview:\n",
    "\n",
    "This script takes in a dataset of fire perimeters and a gridded population dataset. \n",
    "\n",
    "The parameters are: \n",
    "- Area threshold (area_thresh): this is the threshold by which we determine if a fire is large or small. Fires greater than or equal to 1000 acres are considered large. Based on size, fires get different buffers (one buffer for small fires and one for large). This is parameterized in square-meters.\n",
    "- Large fire buffer (large_fire_buffer) = This is the buffer around the fire perimeter for large (>= 1000 acres) fires, in meters.\n",
    "- Small fire buffer (small_fire_buffer) = This is the buffer around the fire perimeter for small (< 1000 acres) fires, in meters.\n",
    "- Population averaging radius (pop_average_radius): This is the radius for a circle with the area that we are using in our denominator of population density. In other words, if our criteria is per 1 square-kilometer, this is the radius of a circle that has an area of 1 square-kilometer (or 1000 square-meters). This is parameterized in meters.\n",
    "- Population density criteria (pop_density_criteria): This is the number of people per square-meter that are required to make an area a community.\n",
    "\n",
    "The process is: \n",
    "1. It first buffers each fire, with a different buffer size based on whether the fire is big or small. A big fire is defined as a fire that is bigger than 1000 acres (converted to 4046856 meters). Big fires get buffered by 20km and small fires get buffered by 10km. \n",
    "\n",
    "2. Following the buffering of the fire, we create a bounding box around the fire that adds the radius of a `pop_average_radius` circle to each side of the fire. This is to ensure we pull enough of the population raster to do our density calculation correctly.\n",
    "\n",
    "3. Once we have all of this information, we load a subset of our population raster using the bounding box that we created in step 2 to determine the subset loaded.\n",
    "\n",
    "4. Using the population data, we build a kernel for the convolution - this is an average kernel.\n",
    "\n",
    "5. We use our kernel to do a convolution -- for every pixel, we sum all the people within our radius of that pixel (currently 300m). The result of this is a raster of population density averaged to people per square kilometer.\n",
    "\n",
    "6. Now, we can use our buffered fire perimeter to find the maximum value of population density for any individual pixel within the buffered perimeter. If any of them exceed our population density criteria (`pop_density_criteria`), then we determine that the fire overlaps with a community that exceeds our population density threshold and thus meets our population density criteria for the fire overlapping with a community. \n",
    "\n",
    "\n",
    "## Outputs: \n",
    "**Primary output**\n",
    "- CSV with disaster_id and density_criteria_met variable\n",
    "  \n",
    "**Secondary outputs**\n",
    "- Parquet file with metadata (disaster_id, density_criteria_met, state, year, buffer_distance, geometry, crs for that fire's utm)\n",
    "- Diagnostic plots\n",
    "\n",
    "## Next steps:\n",
    "**To discuss with Milo**\n",
    "- File of fires with no acreage (all_disaster_with_ics_poo_no_acreage_cont_us_select_vars) - have point location but not fire size.\n",
    "- There are fires from puerto rico\n",
    "  \n",
    "**Coding next steps**\n",
    "- put into executable python script and make error messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa91c54-4f84-4dfb-9e6b-9d2703866eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T20:59:59.148452Z",
     "iopub.status.busy": "2024-08-22T20:59:59.147731Z",
     "iopub.status.idle": "2024-08-22T20:59:59.151686Z",
     "shell.execute_reply": "2024-08-22T20:59:59.151259Z",
     "shell.execute_reply.started": "2024-08-22T20:59:59.148415Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"~/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire-disaster/data/\").expanduser()\n",
    "plot_dir = Path(\"~/Desktop/Desktop/epidemiology_PhD/02_projects/wildfire-disaster/plots/\").expanduser()\n",
    "# list(data_dir.glob(\"*.parquet\"))\n",
    "# list(data_dir.iterdir())\n",
    "mol_crs = \"ESRI: 54009\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e46ee4-7714-4fb2-a4ed-b415caf33fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T21:00:27.195749Z",
     "iopub.status.busy": "2024-08-22T21:00:27.195171Z",
     "iopub.status.idle": "2024-08-22T21:00:27.203115Z",
     "shell.execute_reply": "2024-08-22T21:00:27.202718Z",
     "shell.execute_reply.started": "2024-08-22T21:00:27.195726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper functions\n",
    "\n",
    "def make_convolution_kernel(\n",
    "    pixel_resolution_m: int | float,\n",
    "    radius_m: int | float,\n",
    ") :\n",
    "    radius = int(radius_m // pixel_resolution_m)\n",
    "    y, x = np.ogrid[-radius : radius + 1, -radius : radius + 1]\n",
    "\n",
    "    kernel = (x**2 + y**2 < radius**2).astype(float)\n",
    "    kernel = kernel / kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def make_spatial_average(\n",
    "    tile: rt.RasterArray,\n",
    "    radius: int | float,\n",
    ") :\n",
    "    arr = np.nan_to_num(tile.to_numpy())\n",
    "\n",
    "    kernel = make_convolution_kernel(tile.x_resolution, radius) # tile.x_resolution is pulling the pop raster res, which is 100m in the ghsl data\n",
    "\n",
    "    out_image = oaconvolve(arr, kernel, mode=\"same\")\n",
    "\n",
    "    out_raster = rt.RasterArray(\n",
    "        out_image,\n",
    "        transform=tile.transform,\n",
    "        crs=tile.crs,\n",
    "        no_data_value=tile.no_data_value,\n",
    "    )\n",
    "    return out_raster\n",
    "\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2092a2-d343-41c6-a7a8-d62b48b25e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T21:00:06.697218Z",
     "iopub.status.busy": "2024-08-22T21:00:06.695859Z",
     "iopub.status.idle": "2024-08-22T21:00:07.007280Z",
     "shell.execute_reply": "2024-08-22T21:00:07.006797Z",
     "shell.execute_reply.started": "2024-08-22T21:00:06.697187Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '/Users/laurenwilner/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire-disaster/data/raw/all_disaster_perimeters_buffers_hawaii_dist_select_vars.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/geopandas/io/arrow.py:653\u001b[0m, in \u001b[0;36m_read_parquet_schema_and_metadata\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/parquet/core.py:1371\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[1;32m   1369\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/dataset.py:476\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    479\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    480\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    481\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    482\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    483\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/dataset.py:441\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/laurenwilner/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire-disaster/data/raw/all_disaster_perimeters_buffers_hawaii_dist_select_vars.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read in fires\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fires_hi \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw/all_disaster_perimeters_buffers_hawaii_dist_select_vars.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m fires_ak \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_parquet(data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw/all_disaster_perimeters_buffers_alaska_dist_select_vars.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m fires_conus \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_parquet(data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw/all_disaster_perimeters_buffers_conus_dist_select_vars.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/geopandas/io/arrow.py:751\u001b[0m, in \u001b[0;36m_read_parquet\u001b[0;34m(path, columns, storage_options, bbox, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m filesystem, path \u001b[38;5;241m=\u001b[39m _get_filesystem_path(\n\u001b[1;32m    748\u001b[0m     path, filesystem\u001b[38;5;241m=\u001b[39mfilesystem, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    749\u001b[0m )\n\u001b[1;32m    750\u001b[0m path \u001b[38;5;241m=\u001b[39m _expand_user(path)\n\u001b[0;32m--> 751\u001b[0m schema, metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_read_parquet_schema_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m geo_metadata \u001b[38;5;241m=\u001b[39m _validate_and_decode_metadata(metadata)\n\u001b[1;32m    755\u001b[0m bbox_filter \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    756\u001b[0m     _get_parquet_bbox_filter(geo_metadata, bbox) \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    757\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/geopandas/io/arrow.py:655\u001b[0m, in \u001b[0;36m_read_parquet_schema_and_metadata\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    653\u001b[0m     schema \u001b[38;5;241m=\u001b[39m parquet\u001b[38;5;241m.\u001b[39mParquetDataset(path, filesystem\u001b[38;5;241m=\u001b[39mfilesystem, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m metadata \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/parquet/core.py:2342\u001b[0m, in \u001b[0;36mread_schema\u001b[0;34m(where, memory_map, decryption_properties, filesystem)\u001b[0m\n\u001b[1;32m   2340\u001b[0m file_ctx \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2342\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m where \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx:\n\u001b[1;32m   2345\u001b[0m     file \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[1;32m   2346\u001b[0m         where, memory_map\u001b[38;5;241m=\u001b[39mmemory_map,\n\u001b[1;32m   2347\u001b[0m         decryption_properties\u001b[38;5;241m=\u001b[39mdecryption_properties)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/_fs.pyx:789\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/wf/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/Users/laurenwilner/Desktop/Desktop/epidemiology_PhD/00_repos/wildfire-disaster/data/raw/all_disaster_perimeters_buffers_hawaii_dist_select_vars.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "# read in fires\n",
    "fires_hi = gpd.read_parquet(data_dir / \"raw/all_disaster_perimeters_buffers_hawaii_dist_select_vars.parquet\")\n",
    "fires_ak = gpd.read_parquet(data_dir / \"raw/all_disaster_perimeters_buffers_alaska_dist_select_vars.parquet\")\n",
    "fires_conus = gpd.read_parquet(data_dir / \"raw/all_disaster_perimeters_buffers_conus_dist_select_vars.parquet\")\n",
    "fires_misc = gpd.read_parquet(data_dir / \"raw/all_disaster_with_ics_poo_no_acreage_cont_us_select_vars.parquet\")\n",
    "\n",
    "# read in utm map\n",
    "utm_map = pd.read_csv(data_dir / \"utm_popden.csv\")\n",
    "utm_map['state_list'] = utm_map['states'].apply(lambda s: s.split(','))\n",
    "utm_map = utm_map.explode('state_list').set_index('state_list')['crs'].to_dict()\n",
    "utm_map = {s.strip(): f\"EPSG:{crs}\" for s, crs in utm_map.items()}\n",
    "utm_map[\"PR\"] = \"EPSG:3920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04441683-5f04-4068-9322-73b4a2054483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:39:14.435636Z",
     "iopub.status.busy": "2024-08-20T03:39:14.435251Z",
     "iopub.status.idle": "2024-08-20T03:47:27.464989Z",
     "shell.execute_reply": "2024-08-20T03:47:27.464730Z",
     "shell.execute_reply.started": "2024-08-20T03:39:14.435611Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "large_fire_buffer = 20000 # meters\n",
    "small_fire_buffer = 10000 # meters\n",
    "pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 300 sq m \n",
    "pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "\n",
    "fire_dfs = []\n",
    "for df in [fires_conus, fires_ak, fires_hi]: \n",
    "    df['state'] = df['aggregate_states'].apply(lambda s: s[:2])\n",
    "    df['year'] = df['aggregate_year'].apply(lambda y: round(y / 5)*5)\n",
    "    keep_cols = ['disaster_id_clean', 'year', 'state', 'shape']\n",
    "    row_tuples = list(df[keep_cols].itertuples(index=False, name=None))# [:5]\n",
    "    for disaster_id, year, state, fire_poly in tqdm.tqdm(row_tuples):\n",
    "        fire_crs = utm_map[state]\n",
    "        fire_series = gpd.GeoSeries([fire_poly], crs=df.crs).to_crs(fire_crs)\n",
    "        \n",
    "        # I. buffer fire\n",
    "         # i. convert fire to appropriate UTM\n",
    "         # ii. calc area of fire poly\n",
    "         # iii. create a perimeter around the fire \n",
    "           # large fire is if area > 20000\n",
    "           # buffer_dist = ifelse(size == \"large_fire\", 20000, 10000))\n",
    "         # iv. create a bounding box around that perimeter\n",
    "           # buffer bounding box by half the radius of the kernel over which we are estimating density (so plus 1000/sqrt(pi))\n",
    "         # v. convert the buffered bounding box to molweide projection\n",
    "        buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "        buffered_fire_series = fire_series.buffer(buffer_dist) # buffer dist in meters\n",
    "        \n",
    "        bounding_box = buffered_fire_series.envelope.buffer(pop_average_radius*1.1).to_crs(mol_crs).iloc[0]\n",
    "\n",
    "        # II. load pop data \n",
    "        pop = rt.load_raster(data_dir / f\"raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(fire_crs)\n",
    "        pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "\n",
    "        # III. build kernel and do convolution with kernel - for every pixel, sum all the people within a kilometer of that pixel,\n",
    "          # so kernel should be all 1's and should be 10 pixels wide and 10 pixels tall\n",
    "          # that convolution gives back a raster of pop density averaged to people per sq km\n",
    "        mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "\n",
    "        # IV. determine if this fire meets density criteria by: \n",
    "          # i. take buffered fire poly and mask everything outside of that (set everything outside buffered poly to 0 which you can do w/ raster.mask)\n",
    "          # ii. find max pixel val and if it exceeds your threshold then it overlaps with a communtiy that exceeds the threshold and is marked as TRUE in final csv. \n",
    "        max_pop_density = np.max(mean_pop_density.mask(buffered_fire_series).to_numpy())\n",
    "        density_criteria_met = max_pop_density > pop_density_criteria\n",
    "\n",
    "        # V. add to results df \n",
    "        df_fire = pd.DataFrame({\n",
    "            'disaster_id': [disaster_id],\n",
    "            'density_criteria_met': [density_criteria_met],\n",
    "            'max_pop_density': [max_pop_density],\n",
    "            'state': [state],\n",
    "            'year': [year],\n",
    "            'buffer_distance': [buffer_dist],\n",
    "            'geometry': [fire_series.iloc[0]],\n",
    "            'crs': [fire_crs],\n",
    "            \n",
    "        })\n",
    "        fire_dfs.append(df_fire)\n",
    "\n",
    "df = pd.concat(fire_dfs, ignore_index = True)\n",
    "fires_included_prop = round(len(df[df[\"density_criteria_met\"] == True])/len(df)*100, 2)\n",
    "df['geometry'] = df['geometry'].apply(lambda geom: geom.wkt)\n",
    "df.to_parquet(data_dir / \"processed/fire_pop_density_criteria.parquet\")\n",
    "df[[\"disaster_id\", \"density_criteria_met\"]].to_csv(data_dir / \"processed/fire_pop_density_criteria.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca8eac-6edb-461c-b5d9-8b42c364d8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:57:38.042740Z",
     "iopub.status.busy": "2024-08-20T03:57:38.041859Z",
     "iopub.status.idle": "2024-08-20T03:57:38.239376Z",
     "shell.execute_reply": "2024-08-20T03:57:38.239087Z",
     "shell.execute_reply.started": "2024-08-20T03:57:38.042643Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(data_dir / \"processed/fire_pop_density_criteria.parquet\")\n",
    "disaster_ids = df[\"disaster_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93181529-c947-488a-bf48-7630dc76c89a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:57:39.138135Z",
     "iopub.status.busy": "2024-08-20T03:57:39.137548Z",
     "iopub.status.idle": "2024-08-20T03:57:39.147027Z",
     "shell.execute_reply": "2024-08-20T03:57:39.146526Z",
     "shell.execute_reply.started": "2024-08-20T03:57:39.138107Z"
    }
   },
   "outputs": [],
   "source": [
    "# plots \n",
    "def plot_fires(disaster_id): \n",
    "    area_thresh = 4046856 # 1000 acres converted to sq meters\n",
    "    large_fire_buffer = 20000 # meters\n",
    "    small_fire_buffer = 10000 # meters\n",
    "    pop_average_radius = 300/np.sqrt(np.pi) # radius for a circle with area 1 sq km \n",
    "    pop_density_criteria = 96 # people per sq km, which is 250 people per sq mile\n",
    "    \n",
    "    cmap = \"seismic\"\n",
    "    df_plot = pd.read_parquet(data_dir / \"processed/fire_pop_density_criteria.parquet\").iloc[0]\n",
    "    df_plot = df[df[\"disaster_id\"] == disaster_id].iloc[0]\n",
    "    fire_poly = wkt.loads(df_plot[\"geometry\"])\n",
    "    crs = df_plot['crs']\n",
    "    year = df_plot['year']\n",
    "    \n",
    "    fire_series = gpd.GeoSeries([fire_poly], crs=crs)\n",
    "    buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "    buffered_fire_series = fire_series.buffer(buffer_dist)\n",
    "    fire_poly_plus_buffer = gpd.GeoSeries([fire_poly, buffered_fire_series.iloc[0]], crs=crs)\n",
    "    bounding_box = buffered_fire_series.envelope.buffer(pop_density_radius * 1.1).to_crs(mol_crs).iloc[0]\n",
    "    pop = rt.load_raster(data_dir / f\"raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box).to_crs(crs)\n",
    "    pop_density_per_sq_km = pop * (1000**2 / pop.x_resolution**2)\n",
    "    mean_pop_density = make_spatial_average(pop_density_per_sq_km, pop_average_radius)\n",
    "    \n",
    "    # gridspec layout \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "    \n",
    "    # fig 1: fire poly, buffered fire poly, pop\n",
    "    pop_transformed = pop*100/pop_density_criteria\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    pop_transformed.plot(ax=ax1, cmap = cmap, vmin = 1e-4, vmax = 2, under_color = \"lightgrey\")\n",
    "    fire_poly_plus_buffer.boundary.plot(ax=ax1, linewidth=1.5, edgecolor = 'limegreen')\n",
    "    ax1.set_title(f\"Population counts\", fontsize=16)\n",
    "    ax1.set_axis_off()\n",
    "    \n",
    "    # fig 2: fire poly, buffered fire poly, pop density\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    pop_density_transformed = pop_density/pop_density_criteria\n",
    "    pop_density_transformed.plot(ax=ax2, cmap = cmap, vmin = 1e-4, vmax = 2, under_color = \"lightgrey\")\n",
    "    fire_poly_plus_buffer.boundary.plot(ax=ax2, linewidth=1.5, edgecolor = 'limegreen')\n",
    "    ax2.set_title(f\"Population density\", fontsize=16)\n",
    "    ax2.set_axis_off()\n",
    "    \n",
    "    fig.suptitle(f\"Fire polygon with buffer (buffer distance: {buffer_dist} meters) \\nDisaster ID: {df_plot['disaster_id']}, State: {df_plot['state']}, Year: {df_plot['year']} \\nCriteria met: {df_plot[\"density_criteria_met\"]}\", fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    pdf_filename = plot_dir / f\"fire_pop_dens_{df_plot['disaster_id']}.pdf\"\n",
    "    plt.savefig(pdf_filename, format='pdf')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026f445-6102-4a7e-8be2-65ab0b399fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:57:41.346114Z",
     "iopub.status.busy": "2024-08-20T03:57:41.345569Z",
     "iopub.status.idle": "2024-08-20T03:57:41.661765Z",
     "shell.execute_reply": "2024-08-20T03:57:41.661510Z",
     "shell.execute_reply.started": "2024-08-20T03:57:41.346087Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[2, 1])\n",
    "\n",
    "df_fig = df[df[\"max_pop_density\"]<100000]\n",
    "# fig 1: histogram of max_pop_density for all fires\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "ax1.hist(df_fig['max_pop_density'], bins=150, color='firebrick', edgecolor='black')\n",
    "ax1.set_title('Histogram of maximum population density for all fires, subset to density < 100000')\n",
    "ax1.set_xlabel('Max population density')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# fig 2: histogram of max_pop_density where density_criteria_met = True\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(df_fig[df_fig['density_criteria_met'] == True]['max_pop_density'], bins=150, color='salmon', edgecolor='black')\n",
    "ax2.set_title('Max population density, subset to density < 100000 (criteria met)')\n",
    "ax2.set_xlabel('Max population density')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "# fig 3: histogram of max_pop_density where density_criteria_met = False\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.hist(df_fig[df_fig['density_criteria_met'] == False]['max_pop_density'], bins=150, color='peachpuff', edgecolor='black')\n",
    "ax3.set_title('Max population density (criteria not met)')\n",
    "ax3.set_xlabel('Max population density')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.suptitle(f\"Population density histograms (percent included = {fires_included_prop}%)\", fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "pdf_filename = plot_dir / f\"histograms.pdf\"\n",
    "plt.savefig(pdf_filename, format='pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ad044-f2fd-4212-ac3d-a26e1f4a65fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:57:48.739288Z",
     "iopub.status.busy": "2024-08-20T03:57:48.738661Z",
     "iopub.status.idle": "2024-08-20T04:39:15.373694Z",
     "shell.execute_reply": "2024-08-20T04:39:15.373370Z",
     "shell.execute_reply.started": "2024-08-20T03:57:48.739254Z"
    }
   },
   "outputs": [],
   "source": [
    "for disaster in tqdm.tqdm(disaster_ids): \n",
    "    plot_fires(disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b5253-394c-49f3-875c-3b6138c731ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T04:39:24.760519Z",
     "iopub.status.busy": "2024-08-20T04:39:24.759747Z",
     "iopub.status.idle": "2024-08-20T04:40:34.319551Z",
     "shell.execute_reply": "2024-08-20T04:40:34.319247Z",
     "shell.execute_reply.started": "2024-08-20T04:39:24.760468Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_dir = plot_dir\n",
    "pdf_files = sorted(pdf_dir.glob(\"fire_pop_dens_*.pdf\"))\n",
    "histogram_pdf = pdf_dir / \"histograms.pdf\"\n",
    "output_pdf = pdf_dir / \"compiled/combined_fire_plots.pdf\"\n",
    "pdf_merger = PyPDF2.PdfMerger()\n",
    "\n",
    "# histogram first\n",
    "with open(histogram_pdf, 'rb') as f:\n",
    "    pdf_merger.append(f)\n",
    "    \n",
    "# then loop through each fire's pdf\n",
    "for pdf_file in pdf_files:\n",
    "    with open(pdf_file, 'rb') as f:\n",
    "        pdf_merger.append(f)\n",
    "with open(output_pdf, 'wb') as output_file:\n",
    "    pdf_merger.write(output_file)\n",
    "pdf_merger.close()\n",
    "print(f\"Combined PDF saved as {output_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0ec6e-0f9b-4347-9507-61f27a5b860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plots \n",
    "\n",
    "# def plot_fires(disaster_id): \n",
    "#     cmap = \"RdGy\"\n",
    "#     df_plot = pd.read_parquet(data_dir / \"processed/fire_pop_density_criteria.parquet\").iloc[0]\n",
    "#     df_plot = df[df[\"disaster_id\"] == disaster_id].iloc[0]\n",
    "#     fire_poly = wkt.loads(df_plot[\"geometry\"])\n",
    "#     crs = df_plot['crs']\n",
    "#     fire_series = gpd.GeoSeries([fire_poly], crs=crs)\n",
    "#     buffer_dist = large_fire_buffer if fire_series.area.iloc[0] > area_thresh else small_fire_buffer \n",
    "#     buffered_fire_series = fire_series.buffer(buffer_dist)\n",
    "#     fire_poly_plus_buffer = gpd.GeoSeries([fire_poly, buffered_fire_series.iloc[0]], crs=crs)\n",
    "    \n",
    "#     # gridspec layout \n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "#     gs = gridspec.GridSpec(2, 3, figure=fig, width_ratios=[1, 1, 2], height_ratios=[1, 1])\n",
    "    \n",
    "#     # fig 1: fire_poly\n",
    "#     ax1 = fig.add_subplot(gs[0, 0])\n",
    "#     fire_series.plot(ax=ax1, edgecolor='firebrick', facecolor='none')\n",
    "#     ax1.set_title(f\"Fire Polygon\", fontsize=16)\n",
    "#     ax1.title.set_position([0.5, 1.05])\n",
    "#     ax1.set_axis_off()\n",
    "    \n",
    "#     # fig 2: fire poly with buffer\n",
    "#     ax2 = fig.add_subplot(gs[0, 1])\n",
    "#     fire_poly_plus_buffer.boundary.plot(ax=ax2, linewidth=1.5, edgecolor = 'firebrick')\n",
    "#     ax2.set_title(f\"Fire Polygon with Buffer (Buffer Distance: {buffer_dist} meters)\", fontsize=16)\n",
    "#     ax2.title.set_position([0.5, 1.05])\n",
    "#     ax2.set_axis_off()\n",
    "    \n",
    "#     # fig 3: population with fire poly\n",
    "#     bounding_box = buffered_fire_series.envelope.buffer(pop_density_radius * 1.1).to_crs(mol_crs).iloc[0]\n",
    "#     pop = rt.load_raster(data_dir / f\"raw/pop_data/GHS_POP_E{year}_GLOBE_R2023A_54009_100_V1_0.tif\", bounding_box)\n",
    "#     ax3 = fig.add_subplot(gs[1, 0])\n",
    "#     pop.plot(ax=ax3, cmap='viridis')\n",
    "#     fire_series.to_crs(mol_crs).boundary.plot(ax=ax3, color='firebrick')\n",
    "#     ax3.set_title(\"Population density and fire polygon\", fontsize=16)\n",
    "#     ax3.set_axis_off()\n",
    "    \n",
    "#     # fig 4: pop density in box\n",
    "#     pop_density = make_spatial_sum(pop, pop_density_radius)\n",
    "#     ax4 = fig.add_subplot(gs[1, 1])\n",
    "#     pop_density.plot(ax=ax4, cmap='plasma')\n",
    "#     ax4.set_title(\"Population density in bounding box\", fontsize=16)\n",
    "#     ax4.set_axis_off()\n",
    "    \n",
    "#     # fig 5: pop density in fire poly \n",
    "#     ax5 = fig.add_subplot(gs[:, 2])\n",
    "#     pop_density.mask(fire_series.to_crs(mol_crs)).plot(ax=ax5, cmap='plasma')\n",
    "#     ax5.set_title(\"Population density within fire polygon\", fontsize=16)\n",
    "#     ax5.set_axis_off()\n",
    "#     ax5.set_xlim(fire_series.to_crs(mol_crs).total_bounds[[0, 2]])\n",
    "#     ax5.set_ylim(fire_series.to_crs(mol_crs).total_bounds[[1, 3]])\n",
    "    \n",
    "#     fig.suptitle(f\"Population density criteria for wildfires \\nDisaster ID: {df_plot['disaster_id']}, State: {df_plot['state']}, Year: {df_plot['year']} \\nCriteria met: {df_plot[\"density_criteria_met\"]}\", fontsize=20)\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     pdf_filename = plot_dir / f\"fire_pop_dens_{df_plot['disaster_id']}.pdf\"\n",
    "#     plt.savefig(pdf_filename, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79f22e-64c9-4aed-bb78-9d996fe9e06d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T03:12:29.802578Z",
     "iopub.status.busy": "2024-08-20T03:12:29.802126Z",
     "iopub.status.idle": "2024-08-20T03:12:29.806318Z",
     "shell.execute_reply": "2024-08-20T03:12:29.805870Z",
     "shell.execute_reply.started": "2024-08-20T03:12:29.802548Z"
    }
   },
   "outputs": [],
   "source": [
    "30/300**2 * 1000**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47627e5-fc06-4a3b-a9f3-6dc178446fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
